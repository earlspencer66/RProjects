---
title: "Nearest Neighbours: k-Nearest Neighbours in R (Geeks for Geeks)"
output: html notebook
---

Source: https://www.geeksforgeeks.org/knn-k-nearest-neighbour-algorithm-in-r-from-scratch/

lazy learning algorithm

Procedure:
1. Take dataset
2. Select the value of k
3. Calcuate the euclidean distance
4. Identify the k nearest learning points
5. 

```{r}
library(class)
library(rsample)
library(caret)
```

```{r}
data(iris)

data <- iris
```

```{r}
set.seed(123) # for reproducibility

#stratified splitting (for balanced classes)
data_split <- initial_split(data, prop=0.75, strata=Species)
data_train <- training(data_split)
data_test <- testing(data_split)

#extract labels (important to do AFTER splitting)
train_labels <- data_train$Species
test_labels <- data_test$Species
```

```{r}
#k-NN implementation

#skimr::skim(data)
# take k = sqrt of observations
# take an odd number to avoid ties
# consider k of 3
k <- 3

# k_NN classification
predictions <- knn(train=data_train[,1:4], test = data_test[,1:4], cl = train_labels, k=k)

```

```{r}
# Evaluation
# Confusion matrix
confusion_matrix <- table(predictions, test_labels)
print(confusion_matrix)
```

```{r}
# accuracy
accuracy <- mean(predictions == test_labels)
print(paste("Accuracy:", accuracy))
```

```{r}
# calculate sensitivity, specificiry
confusionMatrix(predictions, test_labels)
```

```{r}
#choosing the best k (tuning)
k_values <-1:20
accuracy_scores <- numeric(length(k_values))

for (i in 1:length(k_values)){
   k <- k_values[i]
   predictions1 <- knn(data_train[,1:4], data_test[,1:4], train_labels, k=k)
   accuracy_scores[i] <- mean(predictions1 == test_labels)
}

# plot the accuracy scores
plot(k_values, accuracy_scores, type="l", xlab="k", ylab= "Accuracy", main="accuracy vs k")

# find the best k
best_k <- k_values[which.max(accuracy_scores)]
print(paste("Best k:", best_k))
print(paste("Best Accuracy:", max(accuracy_scores)))
```